# Distributed Machine Learning with Ray

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgit.sr.ht%2F~hyphaebeast%2Fray-live-training/master?filepath=notebook.ipynb)

## Learning Outcomes

* Basics of distributed machine learning
* Use cases that __Ray__ is especially well-suited to solve
* Differences between **Ray** vs. other frameworks
* Internals of **Ray** and its execution model
* Context of the larger **Ray** ecosystem.

## Resources

* [Ray docs](https://docs.ray.io/en/latest/index.html)
* [Ray cluster](https://docs.ray.io/en/latest/cluster/index.html)
* [anyscale Academy](https://anyscale.com/academy/)
* [Scaling up Machine Learning](https://www.cambridge.org/core/books/scaling-up-machine-learning/6F1331FB8D57B77E5052F0E6956E3FBD)
* [Allreduce (or MPI) vs. Parameter server approaches](https://hunch.net/?p=151364)
* [Dive into Deep Learning](https://d2l.ai/chapter_computational-performance/parameterserver.html)
* [PyTorch distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)
* [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)
* [Horovod: Distributed Deep Learning Framework for TensorFlow](https://eng.uber.com/horovod/)

### References/Papers

* [Ray: A Distributed Framework for Emerging AI Applications](https://arxiv.org/abs/1712.05889)
* [Scaling Distributed Machine Learning with the Parameter Server](https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf)
* [Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf)
* [An Architecture for Parallel Topic Models](https://alex.smola.org/papers/2010/SmoNar10.pdf)
* [Hogwild!: A Lock-Free Approach to Parallelizing StochasticGradient Descent](https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf)
* [DistBelief: Large Scale Distributed Deep Networks](https://research.google/pubs/pub40565/)
* [Parallelized Stochastic Gradient Descent](https://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent)
* [A Reliable Effective Terascale Linear Learning System](https://arxiv.org/abs/1110.4198)